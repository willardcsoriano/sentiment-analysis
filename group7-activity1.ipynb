{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60640740",
   "metadata": {},
   "source": [
    "# Activity 1: Emoji-Based Sentiment Analysis\n",
    "### Group 7: Claire and Willard\n",
    "\n",
    "This notebook addresses the requirements for Activity 1, an emoji-based sentiment analysis project. It is divided into two main parts: Question A, which focuses on training a machine learning model, and Question B, which involves building a real-time sentiment analyzer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52157c3",
   "metadata": {},
   "source": [
    "## Question A: Sentiment Analysis using a Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "99129bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8642c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Dataset Head:\n",
      "   Unnamed: 0  sentiment                                               post\n",
      "0           0          1                             Good morning every one\n",
      "1           1          0  TW: S AssaultActually horrified how many frien...\n",
      "2           2          1  Thanks by has notice of me Greetings : Jossett...\n",
      "3           3          0                      its ending soon aah unhappy üòß\n",
      "4           4          1                               My real time happy üòä\n",
      "\n",
      "Emoticons Dataset Head:\n",
      "   Unnamed: 0 Emoji Unicode codepoint                         Unicode name\n",
      "0           0     üòç           0x1f60d  SMILING FACE WITH HEART-SHAPED EYES\n",
      "1           1     üò≠           0x1f62d                   LOUDLY CRYING FACE\n",
      "2           2     üòò           0x1f618                 FACE THROWING A KISS\n",
      "3           3     üòä           0x1f60a       SMILING FACE WITH SMILING EYES\n",
      "4           4     üòÅ           0x1f601      GRINNING FACE WITH SMILING EYES\n"
     ]
    }
   ],
   "source": [
    "# Load the main dataset for Question A\n",
    "file_path_q_a = \"1k_data_emoji_tweets_senti_posneg.csv\"\n",
    "df_main = pd.read_csv(file_path_q_a)\n",
    "\n",
    "# Load the reference dataset for feature engineering\n",
    "file_path_emoticons = \"15_emoticon_data.csv\"\n",
    "df_emoticons = pd.read_csv(file_path_emoticons)\n",
    "\n",
    "# Display the first 5 rows of each DataFrame to verify they loaded correctly\n",
    "print(\"Main Dataset Head:\")\n",
    "print(df_main.head())\n",
    "print(\"\\nEmoticons Dataset Head:\")\n",
    "print(df_emoticons.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee575575",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a817a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  1000 non-null   int64 \n",
      " 1   sentiment   1000 non-null   int64 \n",
      " 2   post        1000 non-null   object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 23.6+ KB\n",
      "\n",
      "Descriptive Statistics:\n",
      "         Unnamed: 0   sentiment    post\n",
      "count   1000.000000  1000.00000    1000\n",
      "unique          NaN         NaN     999\n",
      "top             NaN         NaN  #NAME?\n",
      "freq            NaN         NaN       2\n",
      "mean     499.500000     0.50000     NaN\n",
      "std      288.819436     0.50025     NaN\n",
      "min        0.000000     0.00000     NaN\n",
      "25%      249.750000     0.00000     NaN\n",
      "50%      499.500000     0.50000     NaN\n",
      "75%      749.250000     1.00000     NaN\n",
      "max      999.000000     1.00000     NaN\n",
      "\n",
      "Missing Values:\n",
      "Unnamed: 0    0\n",
      "sentiment     0\n",
      "post          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get a concise summary of the DataFrame\n",
    "df_main.info()\n",
    "\n",
    "# Display descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df_main.describe(include='all'))\n",
    "\n",
    "# Check for any missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_main.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6ef67",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1d2e4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- After Cleaning ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 998 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  998 non-null    int64 \n",
      " 1   post       998 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 23.4+ KB\n",
      "\n",
      "Missing Values:\n",
      "sentiment    0\n",
      "post         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willard\\AppData\\Local\\Temp\\ipykernel_3808\\92509041.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_main['post'].replace('#NAME?', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Drop the redundant 'Unnamed: 0' column\n",
    "df_main = df_main.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Clean the 'post' column by replacing '#NAME?' with a null value and then dropping the row\n",
    "df_main['post'].replace('#NAME?', np.nan, inplace=True)\n",
    "df_main.dropna(subset=['post'], inplace=True)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"--- After Cleaning ---\")\n",
    "df_main.info()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_main.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51217d91",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98ae5337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Willard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Willard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (must run and complete here)\n",
    "# You only need to run this once per new virtual environment.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed robust preprocessing (emoji replacement + negation).\n",
      "USE_NLTK flag: False\n"
     ]
    }
   ],
   "source": [
    "# Robust preprocessing + diagnostics (drop-in)\n",
    "import re, math\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# --- config ---\n",
    "NEGATION_WORDS = {\n",
    "    \"no\",\"not\",\"don't\",\"never\",\"n't\",\"none\",\"nobody\",\"nothing\",\"neither\",\"nowhere\",\n",
    "    \"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"can't\",\"cannot\",\"won't\",\"shouldn't\",\"couldn't\",\n",
    "    \"doesn't\",\"didn't\",\"don't\"\n",
    "}\n",
    "NEGATION_BREAKS = {'.', '!', '?', ';', ':'}\n",
    "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "# --- attempt to use NLTK if available and able to find resources ---\n",
    "USE_NLTK = False\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    # sanity check: calling these can raise LookupError if data not found\n",
    "    _ = word_tokenize(\"test\")\n",
    "    _ = stopwords.words('english')\n",
    "    USE_NLTK = True\n",
    "except Exception as e:\n",
    "    # If anything fails, we'll use fallback below\n",
    "    USE_NLTK = False\n",
    "\n",
    "# set stopwords for fallback or NLTK\n",
    "if USE_NLTK:\n",
    "    NLTK_STOPWORDS = set(stopwords.words('english'))\n",
    "else:\n",
    "    NLTK_STOPWORDS = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# --- safe negation handler ---\n",
    "def handle_negation_safe(text):\n",
    "    # ... (function body from the second block) ...\n",
    "    # (The body is too long to paste here, but use the one from the second block)\n",
    "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "        return \"\"\n",
    "    s = str(text).lower()\n",
    "    # try NLTK tokenization path\n",
    "    if USE_NLTK:\n",
    "        try:\n",
    "            tokens = word_tokenize(s)\n",
    "            neg_on = False\n",
    "            out = []\n",
    "            for tok in tokens:\n",
    "                if tok in NEGATION_BREAKS:\n",
    "                    neg_on = False\n",
    "                    out.append(tok)\n",
    "                elif tok in NEGATION_WORDS:\n",
    "                    neg_on = True\n",
    "                    out.append(tok)\n",
    "                elif neg_on and tok.isalnum() and tok not in NLTK_STOPWORDS:\n",
    "                    out.append(f\"{tok}_NEG\")\n",
    "                else:\n",
    "                    out.append(tok)\n",
    "            return \" \".join(out)\n",
    "        except Exception:\n",
    "            pass # fall back to regex\n",
    "    \n",
    "    # fallback: regex tokenizer + sklearn stopwords\n",
    "    tokens = TOKEN_RE.findall(s)\n",
    "    neg_on = False\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        if tok in NEGATION_BREAKS:\n",
    "            neg_on = False\n",
    "            out.append(tok)\n",
    "        elif tok in NEGATION_WORDS:\n",
    "            neg_on = True\n",
    "            out.append(tok)\n",
    "        elif neg_on and tok.isalnum() and tok not in NLTK_STOPWORDS:\n",
    "            out.append(f\"{tok}_NEG\")\n",
    "        else:\n",
    "            out.append(tok)\n",
    "    return \" \".join(out)\n",
    "\n",
    "\n",
    "# --- improved emoji replacer (uses df_emoticons list like your original) ---\n",
    "_emojis = []\n",
    "try:\n",
    "    _emojis = df_emoticons['Emoji'].dropna().astype(str).tolist()\n",
    "except Exception:\n",
    "    _emojis = []\n",
    "if len(_emojis) > 0:\n",
    "    _emoji_pattern = re.compile('|'.join(re.escape(e) for e in _emojis))\n",
    "else:\n",
    "    _emoji_pattern = None\n",
    "\n",
    "def replace_emojis_safe(text):\n",
    "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "        return \"\"\n",
    "    t = str(text)\n",
    "    if _emoji_pattern:\n",
    "        return _emoji_pattern.sub('_EMOJI_', t)\n",
    "    return t\n",
    "\n",
    "# --- apply to dataframe ---\n",
    "df_main['post_cleaned'] = df_main['post'].fillna('').apply(replace_emojis_safe)\n",
    "df_main['post_cleaned'] = df_main['post_cleaned'].apply(handle_negation_safe)\n",
    "\n",
    "print(\"Completed robust preprocessing (emoji replacement + negation).\")\n",
    "print(f\"USE_NLTK flag: {USE_NLTK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e5b92",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93552521",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af310486",
   "metadata": {},
   "source": [
    "##### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39628d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df_main['post_cleaned']\n",
    "y = df_main['sentiment']\n",
    "\n",
    "# Split the data into 70% training and 30% for validation + testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the remaining 30% into 15% validation and 15% testing\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32219e",
   "metadata": {},
   "source": [
    "##### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae48327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TF-IDF. The vectorizer is fit only on the training data.\n",
    "# Add n-grams (sequences of words).\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_val_vectorized = vectorizer.transform(X_val)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cf61e",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81ae7855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Validation Accuracy: 0.7733\n",
      "Logistic Regression Validation Accuracy: 0.7467\n",
      "Support Vector Machine (SVC) Validation Accuracy: 0.7867\n"
     ]
    }
   ],
   "source": [
    "# Import additional models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# --- Model 1: Multinomial Naive Bayes ---\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_vectorized, y_train)\n",
    "nb_val_pred = nb_model.predict(X_val_vectorized)\n",
    "nb_accuracy = accuracy_score(y_val, nb_val_pred)\n",
    "print(f\"Multinomial Naive Bayes Validation Accuracy: {nb_accuracy:.4f}\")\n",
    "\n",
    "# --- Model 2: Logistic Regression ---\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_vectorized, y_train)\n",
    "lr_val_pred = lr_model.predict(X_val_vectorized)\n",
    "lr_accuracy = accuracy_score(y_val, lr_val_pred)\n",
    "print(f\"Logistic Regression Validation Accuracy: {lr_accuracy:.4f}\")\n",
    "\n",
    "# --- Model 3: Support Vector Machine (SVC) ---\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train_vectorized, y_train)\n",
    "svc_val_pred = svc_model.predict(X_val_vectorized)\n",
    "svc_accuracy = accuracy_score(y_val, svc_val_pred)\n",
    "print(f\"Support Vector Machine (SVC) Validation Accuracy: {svc_accuracy:.4f}\")\n",
    "\n",
    "# Now, based on these results, select the best-performing model to evaluate on the final test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc05a3d",
   "metadata": {},
   "source": [
    "#### Chosen Model/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "586137cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Test Set Accuracy: 0.8066666666666666\n",
      "Multinomial Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81        74\n",
      "           1       0.82      0.79      0.81        76\n",
      "\n",
      "    accuracy                           0.81       150\n",
      "   macro avg       0.81      0.81      0.81       150\n",
      "weighted avg       0.81      0.81      0.81       150\n",
      "\n",
      "\n",
      "Support Vector Machine (SVC) Test Set Accuracy: 0.8133333333333334\n",
      "Support Vector Machine (SVC) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.77      0.80        74\n",
      "           1       0.79      0.86      0.82        76\n",
      "\n",
      "    accuracy                           0.81       150\n",
      "   macro avg       0.82      0.81      0.81       150\n",
      "weighted avg       0.82      0.81      0.81       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate Multinomial Naive Bayes on the Test Set ---\n",
    "nb_test_pred = nb_model.predict(X_test_vectorized)\n",
    "print(\"Multinomial Naive Bayes Test Set Accuracy:\", accuracy_score(y_test, nb_test_pred))\n",
    "print(\"Multinomial Naive Bayes Classification Report:\")\n",
    "print(classification_report(y_test, nb_test_pred))\n",
    "\n",
    "# --- Evaluate Support Vector Machine (SVC) on the Test Set ---\n",
    "svc_test_pred = svc_model.predict(X_test_vectorized)\n",
    "print(\"\\nSupport Vector Machine (SVC) Test Set Accuracy:\", accuracy_score(y_test, svc_test_pred))\n",
    "print(\"Support Vector Machine (SVC) Classification Report:\")\n",
    "print(classification_report(y_test, svc_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c1c36",
   "metadata": {},
   "source": [
    "## Question B: Real-Time Tweet Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33b1b2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fef77a3b37403c885c826fbb02f848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Tweet:', placeholder='Type your sentence here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689bc47571ff4a41a3ce2c3f8a90327b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Say your Sentiment', style=ButtonStyle(), tooltip='Click to analyze s‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4a9642c32447caa8a29f561945fb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the ipywidgets library\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define a function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    # --- CRITICAL CHANGE: Use the unified preprocess_text function ---\n",
    "    # This function must include BOTH the emoji replacement AND the negation handling\n",
    "    # (assuming it was defined in a previous cell)\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Vectorize the new text\n",
    "    vectorized_text = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Make a prediction using the chosen model (SVC in your current code)\n",
    "    # Ensure this model was trained in a previous cell!\n",
    "    prediction = svc_model.predict(vectorized_text)[0]\n",
    "    \n",
    "    # Convert prediction to a readable sentiment label\n",
    "    sentiment = \"POSITIVE\" if prediction == 1 else \"NEGATIVE\"\n",
    "    \n",
    "    # Display the result\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        # Diagnostic print to confirm preprocessing is working\n",
    "        # print(f\"Preprocessed text: \\\"{processed_text}\\\"\")\n",
    "        print(f\"Your input is: \\\"{text}\\\"\")\n",
    "        print(f\"Your input is of \\\"{sentiment} SENTIMENT\\\"\")\n",
    "\n",
    "# Create the interactive widgets\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your sentence here',\n",
    "    description='Tweet:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Say your Sentiment',\n",
    "    button_style='info',\n",
    "    tooltip='Click to analyze sentiment'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Link the button to the prediction function\n",
    "def on_button_clicked(b):\n",
    "    predict_sentiment(text_input.value)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(text_input, button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423536b",
   "metadata": {},
   "source": [
    "*Some misclassifications occur due to label noise (e.g., the word hate appears in both positive- and negative-labeled tweets). With more data or label cleaning, this would improve.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def6e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_NLTK: False\n",
      "Preprocessed token: hate\n",
      "predict_proba(hate): {np.int64(0): np.float64(0.44697162649833977), np.int64(1): np.float64(0.5530283735016599)}\n",
      "class 0 log-prob for 'hate': -8.643503076113314\n",
      "class 1 log-prob for 'hate': -8.424858210080664\n",
      "Training examples containing 'hate' (post_cleaned): 4\n",
      "                                    post  \\\n",
      "426         I hate throwing up unhappy üòß   \n",
      "743     I hate people who steal my ideas   \n",
      "919  My heart hurts. sad I hate people üòî   \n",
      "980  rt if you hate me crying with joy üòÜ   \n",
      "\n",
      "                                   post_cleaned  sentiment  \n",
      "426          i hate throwing up unhappy _emoji_          0  \n",
      "743            i hate people who steal my ideas          1  \n",
      "919  my heart hurts . sad i hate people _emoji_          0  \n",
      "980   rt if you hate me crying with joy _emoji_          1  \n",
      "\n",
      "If you still see a LookupError after this block, try restarting your kernel and re-running the block above once more.\n"
     ]
    }
   ],
   "source": [
    "# --- Run the diagnostics for the token \"hate\" ---\n",
    "print(\"USE_NLTK:\", USE_NLTK)\n",
    "tok = \"hate\"\n",
    "proc = preprocess_text(tok)\n",
    "print(\"Preprocessed token:\", proc)\n",
    "\n",
    "# Vectorizer/model diagnostics (wrap in try/except so it doesn't crash notebook)\n",
    "try:\n",
    "    vec = vectorizer.transform([proc])\n",
    "    if hasattr(nb_model, \"predict_proba\"):\n",
    "        probs = nb_model.predict_proba(vec)[0]\n",
    "        print(\"predict_proba(hate):\", dict(zip(nb_model.classes_, probs)))\n",
    "    else:\n",
    "        print(\"predict(hate):\", nb_model.predict(vec)[0])\n",
    "\n",
    "    # check token presence in vectorizer's vocabulary (we check the raw token and lowercased)\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    if tok in vocab:\n",
    "        idx = vocab[tok]\n",
    "        for i, cls in enumerate(nb_model.classes_):\n",
    "            print(f\"class {cls} log-prob for 'hate':\", nb_model.feature_log_prob_[i][idx])\n",
    "    elif tok.lower() in vocab:\n",
    "        idx = vocab[tok.lower()]\n",
    "        print(f\"Found '{tok.lower()}' in vocabulary at index {idx}\")\n",
    "        for i, cls in enumerate(nb_model.classes_):\n",
    "            print(f\"class {cls} log-prob for '{tok.lower()}':\", nb_model.feature_log_prob_[i][idx])\n",
    "    else:\n",
    "        print(f\"'{tok}' not found in vectorizer vocabulary. This suggests preprocessing transformed it (e.g. to 'hate_NEG') or token wasn't present during training.\")\n",
    "        # show top tokens around 'hate' by inspecting vocabulary keys that contain hate\n",
    "        keys_with_hate = [k for k in vocab.keys() if 'hate' in k]\n",
    "        print(\"Vocab keys containing 'hate' (sample):\", keys_with_hate[:20])\n",
    "except Exception as e:\n",
    "    print(\"Model/vectorizer diagnostic raised an exception:\", repr(e))\n",
    "\n",
    "# --- also show how many training examples include the token 'hate' (after your post_cleaned) ---\n",
    "try:\n",
    "    mask = df_main['post_cleaned'].astype(str).str.contains(r'\\bhate\\b', case=False, na=False)\n",
    "    print(\"Training examples containing 'hate' (post_cleaned):\", mask.sum())\n",
    "    if mask.sum() > 0:\n",
    "        print(df_main.loc[mask, ['post','post_cleaned','sentiment']].head(10))\n",
    "except Exception as e:\n",
    "    print(\"Error checking training examples:\", repr(e))\n",
    "\n",
    "print(\"\\nIf you still see a LookupError after this block, try restarting your kernel and re-running the block above once more.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
