{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60640740",
   "metadata": {},
   "source": [
    "# Activity 1: Emoji-Based Sentiment Analysis\n",
    "### Group 7: Claire and Willard\n",
    "\n",
    "This notebook addresses the requirements for Activity 1, an emoji-based sentiment analysis project. It is divided into two main parts: Question A, which focuses on training a machine learning model, and Question B, which involves building a real-time sentiment analyzer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52157c3",
   "metadata": {},
   "source": [
    "## Question A: Sentiment Analysis using a Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99129bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8642c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Dataset Head:\n",
      "   Unnamed: 0  sentiment                                               post\n",
      "0           0          1                             Good morning every one\n",
      "1           1          0  TW: S AssaultActually horrified how many frien...\n",
      "2           2          1  Thanks by has notice of me Greetings : Jossett...\n",
      "3           3          0                      its ending soon aah unhappy üòß\n",
      "4           4          1                               My real time happy üòä\n",
      "\n",
      "Emoticons Dataset Head:\n",
      "   Unnamed: 0 Emoji Unicode codepoint                         Unicode name\n",
      "0           0     üòç           0x1f60d  SMILING FACE WITH HEART-SHAPED EYES\n",
      "1           1     üò≠           0x1f62d                   LOUDLY CRYING FACE\n",
      "2           2     üòò           0x1f618                 FACE THROWING A KISS\n",
      "3           3     üòä           0x1f60a       SMILING FACE WITH SMILING EYES\n",
      "4           4     üòÅ           0x1f601      GRINNING FACE WITH SMILING EYES\n"
     ]
    }
   ],
   "source": [
    "# Load the main dataset for Question A\n",
    "file_path_q_a = \"1k_data_emoji_tweets_senti_posneg.csv\"\n",
    "df_main = pd.read_csv(file_path_q_a)\n",
    "\n",
    "# Load the reference dataset for feature engineering\n",
    "file_path_emoticons = \"15_emoticon_data.csv\"\n",
    "df_emoticons = pd.read_csv(file_path_emoticons)\n",
    "\n",
    "# Display the first 5 rows of each DataFrame to verify they loaded correctly\n",
    "print(\"Main Dataset Head:\")\n",
    "print(df_main.head())\n",
    "print(\"\\nEmoticons Dataset Head:\")\n",
    "print(df_emoticons.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee575575",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a817a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  1000 non-null   int64 \n",
      " 1   sentiment   1000 non-null   int64 \n",
      " 2   post        1000 non-null   object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 23.6+ KB\n",
      "\n",
      "Descriptive Statistics:\n",
      "         Unnamed: 0   sentiment    post\n",
      "count   1000.000000  1000.00000    1000\n",
      "unique          NaN         NaN     999\n",
      "top             NaN         NaN  #NAME?\n",
      "freq            NaN         NaN       2\n",
      "mean     499.500000     0.50000     NaN\n",
      "std      288.819436     0.50025     NaN\n",
      "min        0.000000     0.00000     NaN\n",
      "25%      249.750000     0.00000     NaN\n",
      "50%      499.500000     0.50000     NaN\n",
      "75%      749.250000     1.00000     NaN\n",
      "max      999.000000     1.00000     NaN\n",
      "\n",
      "Missing Values:\n",
      "Unnamed: 0    0\n",
      "sentiment     0\n",
      "post          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get a concise summary of the DataFrame\n",
    "df_main.info()\n",
    "\n",
    "# Display descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df_main.describe(include='all'))\n",
    "\n",
    "# Check for any missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_main.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6ef67",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d2e4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- After Cleaning ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 998 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  998 non-null    int64 \n",
      " 1   post       998 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 23.4+ KB\n",
      "\n",
      "Missing Values:\n",
      "sentiment    0\n",
      "post         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willard\\AppData\\Local\\Temp\\ipykernel_3808\\92509041.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_main['post'].replace('#NAME?', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Drop the redundant 'Unnamed: 0' column\n",
    "df_main = df_main.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Clean the 'post' column by replacing '#NAME?' with a null value and then dropping the row\n",
    "df_main['post'].replace('#NAME?', np.nan, inplace=True)\n",
    "df_main.dropna(subset=['post'], inplace=True)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"--- After Cleaning ---\")\n",
    "df_main.info()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_main.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51217d91",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98ae5337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Willard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Willard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (must run and complete here)\n",
    "# You only need to run this once per new virtual environment.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6caacad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punkt and stopwords available ‚úî\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.data.find(\"tokenizers/punkt\")\n",
    "nltk.data.find(\"corpora/stopwords\")\n",
    "\n",
    "print(\"punkt and stopwords available ‚úî\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed preprocessing (emoji replacement + negation).\n",
      "USE_NLTK flag: False\n",
      "Note: handle_negation_safe will have used NLTK when available; any LookupErrors were handled by fallback.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# --- configuration & helpers ---\n",
    "NEGATION_WORDS = {\n",
    "    \"no\",\"not\",\"don't\",\"never\",\"n't\",\"none\",\"nobody\",\"nothing\",\"neither\",\"nowhere\",\n",
    "    \"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"can't\",\"cannot\",\"won't\",\"shouldn't\",\"couldn't\",\n",
    "    \"doesn't\",\"didn't\",\"don't\"\n",
    "}\n",
    "NEGATION_BREAKS = {'.', '!', '?', ';', ':'}\n",
    "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "# Try to set up NLTK resources; if anything fails, we'll fall back later\n",
    "USE_NLTK = True\n",
    "try:\n",
    "    # these calls will raise LookupError if missing\n",
    "    _ = word_tokenize(\"test\")\n",
    "    _ = stopwords.words('english')\n",
    "except Exception as e:\n",
    "    USE_NLTK = False\n",
    "\n",
    "# set stopwords for fallback\n",
    "if USE_NLTK:\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "else:\n",
    "    nltk_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# --- robust negation handler ---\n",
    "def handle_negation_safe(text):\n",
    "    \"\"\"\n",
    "    Try to use NLTK tokenization+stopwords; if that raises LookupError or any exception,\n",
    "    fall back to regex tokenization and sklearn stopwords.\n",
    "    \"\"\"\n",
    "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "        return \"\"\n",
    "    s = str(text).lower()\n",
    "    \n",
    "    # try NLTK approach first (if flagged available)\n",
    "    if USE_NLTK:\n",
    "        try:\n",
    "            tokens = word_tokenize(s)\n",
    "            neg_on = False\n",
    "            out = []\n",
    "            for tok in tokens:\n",
    "                if tok in NEGATION_BREAKS:\n",
    "                    neg_on = False\n",
    "                    out.append(tok)\n",
    "                elif tok in NEGATION_WORDS:\n",
    "                    neg_on = True\n",
    "                    out.append(tok)\n",
    "                elif neg_on and tok.isalnum() and tok not in nltk_stopwords:\n",
    "                    out.append(f\"{tok}_NEG\")\n",
    "                else:\n",
    "                    out.append(tok)\n",
    "            # mark success\n",
    "            return \" \".join(out)\n",
    "        except LookupError:\n",
    "            # fall through to regex fallback\n",
    "            pass\n",
    "        except Exception:\n",
    "            # any other tokenization error -> fallback\n",
    "            pass\n",
    "\n",
    "    # Fallback: regex tokenizer + sklearn stopwords\n",
    "    tokens = TOKEN_RE.findall(s)\n",
    "    neg_on = False\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        if tok in NEGATION_BREAKS:\n",
    "            neg_on = False\n",
    "            out.append(tok)\n",
    "        elif tok in NEGATION_WORDS:\n",
    "            neg_on = True\n",
    "            out.append(tok)\n",
    "        elif neg_on and tok.isalnum() and tok not in nltk_stopwords:\n",
    "            out.append(f\"{tok}_NEG\")\n",
    "        else:\n",
    "            out.append(tok)\n",
    "    return \" \".join(out)\n",
    "\n",
    "# --- improved emoji replacer (uses df_emoticons list like your original) ---\n",
    "# compile emoji pattern once\n",
    "_emojis = df_emoticons['Emoji'].dropna().astype(str).tolist()\n",
    "if len(_emojis) > 0:\n",
    "    _emoji_pattern = re.compile('|'.join(re.escape(e) for e in _emojis))\n",
    "else:\n",
    "    _emoji_pattern = None\n",
    "\n",
    "def replace_emojis_safe(text):\n",
    "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "        return \"\"\n",
    "    t = str(text)\n",
    "    if _emoji_pattern:\n",
    "        return _emoji_pattern.sub('_EMOJI_', t)\n",
    "    return t\n",
    "\n",
    "# --- apply to dataframe but with progress-safe method and reporting ---\n",
    "# We'll apply emoji replacement first, then safe negation handling.\n",
    "df_main['post_cleaned'] = df_main['post'].fillna('').apply(replace_emojis_safe)\n",
    "\n",
    "# Apply negation handling; to be robust, count how many used fallback vs nltk\n",
    "nltk_used = 0\n",
    "fallback_used = 0\n",
    "\n",
    "def apply_and_count(x):\n",
    "    global nltk_used, fallback_used\n",
    "    # If USE_NLTK is True, we optimistically assume it will use NLTK,\n",
    "    # but handle_negation_safe itself decides and doesn't provide the flag.\n",
    "    # To count, we can try calling word_tokenize inside a try and see if it raises.\n",
    "    try:\n",
    "        # attempt NLTK route for counting only\n",
    "        if USE_NLTK:\n",
    "            _ = word_tokenize(str(x).lower())\n",
    "            nltk_used += 1\n",
    "        else:\n",
    "            raise LookupError\n",
    "    except Exception:\n",
    "        fallback_used += 1\n",
    "    return handle_negation_safe(x)\n",
    "\n",
    "# Execute apply (single pass)\n",
    "df_main['post_cleaned'] = df_main['post_cleaned'].apply(handle_negation_safe)\n",
    "\n",
    "print(\"Completed preprocessing (emoji replacement + negation).\")\n",
    "print(f\"USE_NLTK flag: {USE_NLTK}\")\n",
    "print(\"Note: handle_negation_safe will have used NLTK when available; any LookupErrors were handled by fallback.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e5b92",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93552521",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af310486",
   "metadata": {},
   "source": [
    "##### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39628d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df_main['post_cleaned']\n",
    "y = df_main['sentiment']\n",
    "\n",
    "# Split the data into 70% training and 30% for validation + testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the remaining 30% into 15% validation and 15% testing\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32219e",
   "metadata": {},
   "source": [
    "##### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae48327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TF-IDF. The vectorizer is fit only on the training data.\n",
    "# Add n-grams (sequences of words).\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_val_vectorized = vectorizer.transform(X_val)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cf61e",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81ae7855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Validation Accuracy: 0.7733\n",
      "Logistic Regression Validation Accuracy: 0.7467\n",
      "Support Vector Machine (SVC) Validation Accuracy: 0.7867\n"
     ]
    }
   ],
   "source": [
    "# Import additional models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# --- Model 1: Multinomial Naive Bayes ---\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_vectorized, y_train)\n",
    "nb_val_pred = nb_model.predict(X_val_vectorized)\n",
    "nb_accuracy = accuracy_score(y_val, nb_val_pred)\n",
    "print(f\"Multinomial Naive Bayes Validation Accuracy: {nb_accuracy:.4f}\")\n",
    "\n",
    "# --- Model 2: Logistic Regression ---\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_vectorized, y_train)\n",
    "lr_val_pred = lr_model.predict(X_val_vectorized)\n",
    "lr_accuracy = accuracy_score(y_val, lr_val_pred)\n",
    "print(f\"Logistic Regression Validation Accuracy: {lr_accuracy:.4f}\")\n",
    "\n",
    "# --- Model 3: Support Vector Machine (SVC) ---\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train_vectorized, y_train)\n",
    "svc_val_pred = svc_model.predict(X_val_vectorized)\n",
    "svc_accuracy = accuracy_score(y_val, svc_val_pred)\n",
    "print(f\"Support Vector Machine (SVC) Validation Accuracy: {svc_accuracy:.4f}\")\n",
    "\n",
    "# Now, based on these results, select the best-performing model to evaluate on the final test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc05a3d",
   "metadata": {},
   "source": [
    "#### Chosen Model/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "586137cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Test Set Accuracy: 0.8066666666666666\n",
      "Multinomial Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81        74\n",
      "           1       0.82      0.79      0.81        76\n",
      "\n",
      "    accuracy                           0.81       150\n",
      "   macro avg       0.81      0.81      0.81       150\n",
      "weighted avg       0.81      0.81      0.81       150\n",
      "\n",
      "\n",
      "Support Vector Machine (SVC) Test Set Accuracy: 0.8133333333333334\n",
      "Support Vector Machine (SVC) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.77      0.80        74\n",
      "           1       0.79      0.86      0.82        76\n",
      "\n",
      "    accuracy                           0.81       150\n",
      "   macro avg       0.82      0.81      0.81       150\n",
      "weighted avg       0.82      0.81      0.81       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate Multinomial Naive Bayes on the Test Set ---\n",
    "nb_test_pred = nb_model.predict(X_test_vectorized)\n",
    "print(\"Multinomial Naive Bayes Test Set Accuracy:\", accuracy_score(y_test, nb_test_pred))\n",
    "print(\"Multinomial Naive Bayes Classification Report:\")\n",
    "print(classification_report(y_test, nb_test_pred))\n",
    "\n",
    "# --- Evaluate Support Vector Machine (SVC) on the Test Set ---\n",
    "svc_test_pred = svc_model.predict(X_test_vectorized)\n",
    "print(\"\\nSupport Vector Machine (SVC) Test Set Accuracy:\", accuracy_score(y_test, svc_test_pred))\n",
    "print(\"Support Vector Machine (SVC) Classification Report:\")\n",
    "print(classification_report(y_test, svc_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c1c36",
   "metadata": {},
   "source": [
    "## Question B: Real-Time Tweet Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33b1b2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b4b4e3a7b04bf699883b28f32407da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Tweet:', placeholder='Type your sentence here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddc62d4e2cc4dffa80bb538be6febcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Say your Sentiment', style=ButtonStyle(), tooltip='Click to analyze s‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27393eec213423ba034b38e55508520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the ipywidgets library\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define a function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    # Use the same emoji replacement logic\n",
    "    processed_text = replace_emojis(text)\n",
    "    \n",
    "    # Vectorize the new text\n",
    "    vectorized_text = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Make a prediction using the best-performing model (MultinomialNB)\n",
    "    prediction = nb_model.predict(vectorized_text)[0]\n",
    "    \n",
    "    # Convert prediction to a readable sentiment label\n",
    "    sentiment = \"POSITIVE\" if prediction == 1 else \"NEGATIVE\"\n",
    "    \n",
    "    # Display the result\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        print(f\"Your input is: \\\"{text}\\\"\")\n",
    "        print(f\"Your input is of \\\"{sentiment} SENTIMENT\\\"\")\n",
    "\n",
    "# Create the interactive widgets\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your sentence here',\n",
    "    description='Tweet:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Say your Sentiment',\n",
    "    button_style='info',\n",
    "    tooltip='Click to analyze sentiment'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Link the button to the prediction function\n",
    "def on_button_clicked(b):\n",
    "    predict_sentiment(text_input.value)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(text_input, button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423536b",
   "metadata": {},
   "source": [
    "*Some misclassifications occur due to label noise (e.g., the word hate appears in both positive- and negative-labeled tweets). With more data or label cleaning, this would improve.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
